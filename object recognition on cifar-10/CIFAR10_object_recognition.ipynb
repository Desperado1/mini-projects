{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Object Recognition (Cifar-10 dataset)\n",
    "\n",
    "## Importing required packages\n",
    "\n",
    "here i have imported all the important packages required for the object recognition task.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from keras import applications\n",
    "from keras.models import Sequential, Model \n",
    "from keras import optimizers\n",
    "import sys\n",
    "from matplotlib import pyplot\n",
    "from keras.datasets import cifar10\n",
    "from keras.utils import to_categorical\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.optimizers import Adam\n",
    "import h5py\n",
    "from keras.callbacks import ModelCheckpoint   \n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.optimizers import SGD\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import BatchNormalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset\n",
    "\n",
    "Cifar-10 dataset is made available by keras through its Datasets packages. I have used this package just to save the hassle of downloading the dataset and then converting it to required size. \n",
    "\n",
    "after loading the data i have defined a method prep_pixels() so as to normalize and rescale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading train and test dataset\n",
    "def load_dataset():\n",
    "    (trainX, trainY), (testX, testY) = cifar10.load_data()\n",
    "    # one hot encode target values\n",
    "    trainY = to_categorical(trainY)\n",
    "    testY = to_categorical(testY)\n",
    "    return trainX, trainY, testX, testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_pixels(train, test):\n",
    "    train_norm = train.astype('float32')\n",
    "    test_norm = test.astype('float32')\n",
    "\n",
    "    train_norm = train_norm / 255.0\n",
    "    test_norm = test_norm / 255.0\n",
    "    return train_norm, test_norm\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FIRST APPROACH : defining the model (Transfer learning aproach)\n",
    "\n",
    "I am using the VGG19 model for the task. VGG19 has 19 layer arhitecture. here i decided to go with not reshaping the original size of the image provided in the cifar-10 dataset and as a result i have kept all the layers of the network as trainable.\n",
    "\n",
    "I have given include_top parameter as False so i am not using the Fully Connected layers of VGG19 model and thus i have defined my own Dense(fully connected) layers with last layers giving 10 outputs i.e the no. of classes we have in cifar-10 dataset.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.engine.training.Model at 0x7f4fad267908>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# define model\n",
    "def define_model():\n",
    "    model = applications.VGG19(weights = 'imagenet', include_top = False, input_shape = (32, 32, 3))\n",
    "    \"\"\"for layer in model.layers:\n",
    "        layer.trainable = False\"\"\" \n",
    "    x = model.output\n",
    "    x = Flatten()(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    x = Dropout(0.4)(x)\n",
    "    x = Dense(1024, activation=\"relu\")(x)\n",
    "    predictions = Dense(10, activation=\"softmax\")(x)\n",
    "    model_final = Model(input = model.input, output = predictions)\n",
    "    model.summary()\n",
    "    model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.001, momentum=0.9), metrics=[\"accuracy\"])\n",
    "\n",
    "    return model_final\n",
    "define_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plotting loss and accuracy (for test and validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# plot diagnostic learning curves\n",
    "def summarize_diagnostics(history):\n",
    "    # plot loss\n",
    "    pyplot.subplot(211)\n",
    "    pyplot.title('Cross Entropy Loss')\n",
    "    pyplot.plot(history.history['loss'], color='blue', label='train')\n",
    "    pyplot.plot(history.history['val_loss'], color='black', label='val')\n",
    "\n",
    "    # plot accuracy\n",
    "    pyplot.subplot(212)\n",
    "    pyplot.title('Classification Accuracy')\n",
    "    pyplot.plot(history.history['acc'], color='blue', label='train')\n",
    "    pyplot.plot(history.history['val_acc'], color='black', label='val')\n",
    "\n",
    "    pyplot.show()\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## assembling the methods\n",
    "\n",
    "creating a skelton method for all the method defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the test harness for evaluating a model\n",
    "def Skeleton():\n",
    "    # load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    print(\"Data loaded\")\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels(trainX, testX)\n",
    "    print(\"prepared pixels\")\n",
    "    # define model\n",
    "    model = define_model()\n",
    "    print(\"model defined\")\n",
    "    \n",
    "    # fit model\n",
    "    checkpointer = ModelCheckpoint(filepath='model_best.h5', \n",
    "                               verbose=1, save_best_only=True, save_weights_only = True)    \n",
    "    history = model.fit(trainX, trainY, epochs=30, batch_size=64, validation_split = 0.2,\n",
    "                        callbacks = [checkpointer], verbose=1, shuffle = True)\n",
    "    print(\".....\")\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate(testX, testY, verbose=1)\n",
    "    print('> %.3f' % (acc * 100.0))\n",
    "    # learning curves\n",
    "    summarize_diagnostics(history)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n",
      "prepared pixels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Model` call to the Keras 2 API: `Model(inputs=Tensor(\"in..., outputs=Tensor(\"de...)`\n",
      "  if sys.path[0] == '':\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         (None, 32, 32, 3)         0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 32, 32, 64)        1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 32, 32, 64)        36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 16, 16, 64)        0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 16, 16, 128)       73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 16, 16, 128)       147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 8, 8, 128)         0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 8, 8, 256)         295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_conv4 (Conv2D)        (None, 8, 8, 256)         590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 4, 4, 256)         0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 4, 4, 512)         1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv4 (Conv2D)        (None, 4, 4, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 2, 2, 512)         0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv4 (Conv2D)        (None, 2, 2, 512)         2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 1, 1, 512)         0         \n",
      "=================================================================\n",
      "Total params: 20,024,384\n",
      "Trainable params: 20,024,384\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "model defined\n",
      "Train on 40000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "40000/40000 [==============================] - 69s 2ms/step - loss: 1.1458 - acc: 0.5948 - val_loss: 0.7180 - val_acc: 0.7527\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.71801, saving model to model_best.h5\n",
      "Epoch 2/30\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.7018 - acc: 0.7607 - val_loss: 0.6487 - val_acc: 0.7741\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.71801 to 0.64866, saving model to model_best.h5\n",
      "Epoch 3/30\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.5580 - acc: 0.8075 - val_loss: 0.5652 - val_acc: 0.8088\n",
      "\n",
      "Epoch 00003: val_loss improved from 0.64866 to 0.56516, saving model to model_best.h5\n",
      "Epoch 4/30\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.4720 - acc: 0.8396 - val_loss: 0.5215 - val_acc: 0.8223\n",
      "\n",
      "Epoch 00004: val_loss improved from 0.56516 to 0.52147, saving model to model_best.h5\n",
      "Epoch 5/30\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.3941 - acc: 0.8642 - val_loss: 0.5075 - val_acc: 0.8299\n",
      "\n",
      "Epoch 00005: val_loss improved from 0.52147 to 0.50753, saving model to model_best.h5\n",
      "Epoch 6/30\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.3322 - acc: 0.8869 - val_loss: 0.5049 - val_acc: 0.8347\n",
      "\n",
      "Epoch 00006: val_loss improved from 0.50753 to 0.50490, saving model to model_best.h5\n",
      "Epoch 7/30\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.2763 - acc: 0.9052 - val_loss: 0.4980 - val_acc: 0.8358\n",
      "\n",
      "Epoch 00007: val_loss improved from 0.50490 to 0.49801, saving model to model_best.h5\n",
      "Epoch 8/30\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.2309 - acc: 0.9222 - val_loss: 0.4965 - val_acc: 0.8463\n",
      "\n",
      "Epoch 00008: val_loss improved from 0.49801 to 0.49652, saving model to model_best.h5\n",
      "Epoch 9/30\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.1831 - acc: 0.9378 - val_loss: 0.5127 - val_acc: 0.8477\n",
      "\n",
      "Epoch 00009: val_loss did not improve from 0.49652\n",
      "Epoch 10/30\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.1441 - acc: 0.9517 - val_loss: 0.5444 - val_acc: 0.8488\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 0.49652\n",
      "Epoch 11/30\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.1194 - acc: 0.9587 - val_loss: 0.6466 - val_acc: 0.8396\n",
      "\n",
      "Epoch 00011: val_loss did not improve from 0.49652\n",
      "Epoch 12/30\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.0969 - acc: 0.9670 - val_loss: 0.6428 - val_acc: 0.8396\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 0.49652\n",
      "Epoch 13/30\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.0714 - acc: 0.9774 - val_loss: 0.6734 - val_acc: 0.8446\n",
      "\n",
      "Epoch 00013: val_loss did not improve from 0.49652\n",
      "Epoch 14/30\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.0636 - acc: 0.9789 - val_loss: 0.8324 - val_acc: 0.8123\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.49652\n",
      "Epoch 15/30\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.0526 - acc: 0.9823 - val_loss: 0.7183 - val_acc: 0.8443\n",
      "\n",
      "Epoch 00015: val_loss did not improve from 0.49652\n",
      "Epoch 16/30\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.0410 - acc: 0.9866 - val_loss: 0.7657 - val_acc: 0.8389\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.49652\n",
      "Epoch 17/30\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.0389 - acc: 0.9872 - val_loss: 0.7374 - val_acc: 0.8467\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.49652\n",
      "Epoch 18/30\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.0263 - acc: 0.9914 - val_loss: 0.7509 - val_acc: 0.8494\n",
      "\n",
      "Epoch 00018: val_loss did not improve from 0.49652\n",
      "Epoch 19/30\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.0285 - acc: 0.9901 - val_loss: 0.7614 - val_acc: 0.8479\n",
      "\n",
      "Epoch 00019: val_loss did not improve from 0.49652\n",
      "Epoch 20/30\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.0245 - acc: 0.9920 - val_loss: 0.8458 - val_acc: 0.8497\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.49652\n",
      "Epoch 21/30\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.0263 - acc: 0.9908 - val_loss: 0.7473 - val_acc: 0.8481\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.49652\n",
      "Epoch 22/30\n",
      "40000/40000 [==============================] - 66s 2ms/step - loss: 0.0243 - acc: 0.9923 - val_loss: 0.8002 - val_acc: 0.8572\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.49652\n",
      "Epoch 23/30\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.0121 - acc: 0.9962 - val_loss: 0.8287 - val_acc: 0.8508\n",
      "\n",
      "Epoch 00023: val_loss did not improve from 0.49652\n",
      "Epoch 24/30\n",
      "40000/40000 [==============================] - 66s 2ms/step - loss: 0.0168 - acc: 0.9945 - val_loss: 0.8199 - val_acc: 0.8537\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.49652\n",
      "Epoch 25/30\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.0185 - acc: 0.9937 - val_loss: 0.8120 - val_acc: 0.8495\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.49652\n",
      "Epoch 26/30\n",
      "40000/40000 [==============================] - 66s 2ms/step - loss: 0.0121 - acc: 0.9960 - val_loss: 0.8911 - val_acc: 0.8425\n",
      "\n",
      "Epoch 00026: val_loss did not improve from 0.49652\n",
      "Epoch 27/30\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.0155 - acc: 0.9950 - val_loss: 0.8917 - val_acc: 0.8450\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.49652\n",
      "Epoch 28/30\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.0080 - acc: 0.9976 - val_loss: 0.8674 - val_acc: 0.8522\n",
      "\n",
      "Epoch 00028: val_loss did not improve from 0.49652\n",
      "Epoch 29/30\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.0163 - acc: 0.9946 - val_loss: 0.8342 - val_acc: 0.8537\n",
      "\n",
      "Epoch 00029: val_loss did not improve from 0.49652\n",
      "Epoch 30/30\n",
      "40000/40000 [==============================] - 67s 2ms/step - loss: 0.0094 - acc: 0.9970 - val_loss: 0.8750 - val_acc: 0.8492\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.49652\n",
      ".....\n",
      "10000/10000 [==============================] - 7s 671us/step\n",
      "> 84.200\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Skeleton()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SECOND APPROACH: CNN with data augmentation and batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model_2():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(32, 32, 3)))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.2))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(MaxPooling2D((2, 2)))\n",
    "    model.add(Dropout(0.4))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(128, activation='relu', kernel_initializer='he_uniform'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(10, activation='softmax'))\n",
    "    opt = SGD(lr=0.001, momentum=0.9)\n",
    "    model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run the test harness for evaluating a model\n",
    "def Skeleton_2():\n",
    "    # load dataset\n",
    "    trainX, trainY, testX, testY = load_dataset()\n",
    "    print(\"Data loaded\")\n",
    "    # prepare pixel data\n",
    "    trainX, testX = prep_pixels(trainX, testX)\n",
    "    print(\"prepared pixels\")\n",
    "    # define model\n",
    "    model = define_model_2()\n",
    "    print(\"model defined\")\n",
    "    # fit model\n",
    "    checkpointer = ModelCheckpoint(filepath='model_best1.h5', \n",
    "                               verbose=1, save_best_only=True, save_weights_only=True)    \n",
    "    datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n",
    "\n",
    "    it_train = datagen.flow(trainX, trainY, batch_size=64)\n",
    "    # fit model\n",
    "    steps = int(trainX.shape[0] / 64)\n",
    "    history = model.fit_generator(it_train, steps_per_epoch=steps, epochs=200,\n",
    "                                  validation_data=(testX, testY), callbacks=[checkpointer], verbose=1)\n",
    "    # evaluate model\n",
    "    _, acc = model.evaluate(testX, testY, verbose=1)\n",
    "    print('> %.3f' % (acc * 100.0))\n",
    "    # learning curves\n",
    "    summarize_diagnostics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0630 12:19:42.017114 140277725554432 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:74: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
      "\n",
      "W0630 12:19:42.048548 140277725554432 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:517: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "W0630 12:19:42.055153 140277725554432 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:4138: The name tf.random_uniform is deprecated. Please use tf.random.uniform instead.\n",
      "\n",
      "W0630 12:19:42.090164 140277725554432 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:174: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
      "\n",
      "W0630 12:19:42.090868 140277725554432 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:181: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepared pixels\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0630 12:19:45.507943 140277725554432 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:1834: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n",
      "W0630 12:19:45.674919 140277725554432 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3976: The name tf.nn.max_pool is deprecated. Please use tf.nn.max_pool2d instead.\n",
      "\n",
      "W0630 12:19:45.682949 140277725554432 deprecation.py:506] From /home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/backend/tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "W0630 12:19:46.370903 140277725554432 deprecation_wrapper.py:119] From /home/ubuntu/anaconda3/lib/python3.6/site-packages/keras/optimizers.py:790: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
      "\n",
      "W0630 12:19:46.489403 140277725554432 deprecation.py:323] From /home/ubuntu/anaconda3/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model defined\n",
      "Epoch 1/200\n",
      "781/781 [==============================] - 34s 44ms/step - loss: 2.1412 - acc: 0.2974 - val_loss: 1.4973 - val_acc: 0.4450\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 1.49734, saving model to model_best1.h5\n",
      "Epoch 2/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 1.6395 - acc: 0.3985 - val_loss: 1.3879 - val_acc: 0.4920\n",
      "\n",
      "Epoch 00002: val_loss improved from 1.49734 to 1.38785, saving model to model_best1.h5\n",
      "Epoch 3/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 1.5113 - acc: 0.4458 - val_loss: 1.3797 - val_acc: 0.4920\n",
      "\n",
      "Epoch 00003: val_loss improved from 1.38785 to 1.37969, saving model to model_best1.h5\n",
      "Epoch 4/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 1.4305 - acc: 0.4795 - val_loss: 1.3297 - val_acc: 0.5164\n",
      "\n",
      "Epoch 00004: val_loss improved from 1.37969 to 1.32971, saving model to model_best1.h5\n",
      "Epoch 5/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 1.3678 - acc: 0.5023 - val_loss: 1.2956 - val_acc: 0.5271\n",
      "\n",
      "Epoch 00005: val_loss improved from 1.32971 to 1.29556, saving model to model_best1.h5\n",
      "Epoch 6/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 1.3147 - acc: 0.5246 - val_loss: 1.2333 - val_acc: 0.5540\n",
      "\n",
      "Epoch 00006: val_loss improved from 1.29556 to 1.23325, saving model to model_best1.h5\n",
      "Epoch 7/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 1.2693 - acc: 0.5424 - val_loss: 1.1362 - val_acc: 0.5932\n",
      "\n",
      "Epoch 00007: val_loss improved from 1.23325 to 1.13622, saving model to model_best1.h5\n",
      "Epoch 8/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 1.2245 - acc: 0.5576 - val_loss: 1.1702 - val_acc: 0.5825\n",
      "\n",
      "Epoch 00008: val_loss did not improve from 1.13622\n",
      "Epoch 9/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 1.1930 - acc: 0.5704 - val_loss: 1.0762 - val_acc: 0.6110\n",
      "\n",
      "Epoch 00009: val_loss improved from 1.13622 to 1.07621, saving model to model_best1.h5\n",
      "Epoch 10/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 1.1607 - acc: 0.5821 - val_loss: 1.1116 - val_acc: 0.6023\n",
      "\n",
      "Epoch 00010: val_loss did not improve from 1.07621\n",
      "Epoch 11/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 1.1343 - acc: 0.5953 - val_loss: 1.0177 - val_acc: 0.6330\n",
      "\n",
      "Epoch 00011: val_loss improved from 1.07621 to 1.01767, saving model to model_best1.h5\n",
      "Epoch 12/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 1.1137 - acc: 0.6030 - val_loss: 1.1155 - val_acc: 0.6069\n",
      "\n",
      "Epoch 00012: val_loss did not improve from 1.01767\n",
      "Epoch 13/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 1.0869 - acc: 0.6132 - val_loss: 0.9483 - val_acc: 0.6578\n",
      "\n",
      "Epoch 00013: val_loss improved from 1.01767 to 0.94829, saving model to model_best1.h5\n",
      "Epoch 14/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 1.0622 - acc: 0.6231 - val_loss: 0.9526 - val_acc: 0.6605\n",
      "\n",
      "Epoch 00014: val_loss did not improve from 0.94829\n",
      "Epoch 15/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 1.0452 - acc: 0.6291 - val_loss: 0.9429 - val_acc: 0.6665\n",
      "\n",
      "Epoch 00015: val_loss improved from 0.94829 to 0.94286, saving model to model_best1.h5\n",
      "Epoch 16/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 1.0296 - acc: 0.6340 - val_loss: 1.1196 - val_acc: 0.6153\n",
      "\n",
      "Epoch 00016: val_loss did not improve from 0.94286\n",
      "Epoch 17/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 1.0200 - acc: 0.6392 - val_loss: 0.9840 - val_acc: 0.6509\n",
      "\n",
      "Epoch 00017: val_loss did not improve from 0.94286\n",
      "Epoch 18/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.9900 - acc: 0.6508 - val_loss: 0.9399 - val_acc: 0.6658\n",
      "\n",
      "Epoch 00018: val_loss improved from 0.94286 to 0.93988, saving model to model_best1.h5\n",
      "Epoch 19/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.9734 - acc: 0.6550 - val_loss: 0.9083 - val_acc: 0.6780\n",
      "\n",
      "Epoch 00019: val_loss improved from 0.93988 to 0.90825, saving model to model_best1.h5\n",
      "Epoch 20/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.9716 - acc: 0.6558 - val_loss: 0.9084 - val_acc: 0.6726\n",
      "\n",
      "Epoch 00020: val_loss did not improve from 0.90825\n",
      "Epoch 21/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.9506 - acc: 0.6647 - val_loss: 0.9218 - val_acc: 0.6762\n",
      "\n",
      "Epoch 00021: val_loss did not improve from 0.90825\n",
      "Epoch 22/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.9369 - acc: 0.6701 - val_loss: 1.0400 - val_acc: 0.6366\n",
      "\n",
      "Epoch 00022: val_loss did not improve from 0.90825\n",
      "Epoch 23/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.9242 - acc: 0.6754 - val_loss: 0.8931 - val_acc: 0.6835\n",
      "\n",
      "Epoch 00023: val_loss improved from 0.90825 to 0.89314, saving model to model_best1.h5\n",
      "Epoch 24/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.9088 - acc: 0.6808 - val_loss: 0.8955 - val_acc: 0.6814\n",
      "\n",
      "Epoch 00024: val_loss did not improve from 0.89314\n",
      "Epoch 25/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.8997 - acc: 0.6840 - val_loss: 0.8932 - val_acc: 0.6891\n",
      "\n",
      "Epoch 00025: val_loss did not improve from 0.89314\n",
      "Epoch 26/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.8836 - acc: 0.6896 - val_loss: 0.8395 - val_acc: 0.7013\n",
      "\n",
      "Epoch 00026: val_loss improved from 0.89314 to 0.83945, saving model to model_best1.h5\n",
      "Epoch 27/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.8855 - acc: 0.6885 - val_loss: 0.8581 - val_acc: 0.7003\n",
      "\n",
      "Epoch 00027: val_loss did not improve from 0.83945\n",
      "Epoch 28/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.8721 - acc: 0.6957 - val_loss: 0.8025 - val_acc: 0.7164\n",
      "\n",
      "Epoch 00028: val_loss improved from 0.83945 to 0.80247, saving model to model_best1.h5\n",
      "Epoch 29/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.8625 - acc: 0.6988 - val_loss: 0.7979 - val_acc: 0.7135\n",
      "\n",
      "Epoch 00029: val_loss improved from 0.80247 to 0.79790, saving model to model_best1.h5\n",
      "Epoch 30/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.8528 - acc: 0.7016 - val_loss: 0.8247 - val_acc: 0.7104\n",
      "\n",
      "Epoch 00030: val_loss did not improve from 0.79790\n",
      "Epoch 31/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.8426 - acc: 0.7065 - val_loss: 0.7564 - val_acc: 0.7314\n",
      "\n",
      "Epoch 00031: val_loss improved from 0.79790 to 0.75637, saving model to model_best1.h5\n",
      "Epoch 32/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.8355 - acc: 0.7080 - val_loss: 0.8190 - val_acc: 0.7117\n",
      "\n",
      "Epoch 00032: val_loss did not improve from 0.75637\n",
      "Epoch 33/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.8246 - acc: 0.7116 - val_loss: 0.7576 - val_acc: 0.7298\n",
      "\n",
      "Epoch 00033: val_loss did not improve from 0.75637\n",
      "Epoch 34/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.8154 - acc: 0.7145 - val_loss: 0.7591 - val_acc: 0.7294\n",
      "\n",
      "Epoch 00034: val_loss did not improve from 0.75637\n",
      "Epoch 35/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.8113 - acc: 0.7170 - val_loss: 0.7689 - val_acc: 0.7256\n",
      "\n",
      "Epoch 00035: val_loss did not improve from 0.75637\n",
      "Epoch 36/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.8009 - acc: 0.7193 - val_loss: 0.7156 - val_acc: 0.7498\n",
      "\n",
      "Epoch 00036: val_loss improved from 0.75637 to 0.71559, saving model to model_best1.h5\n",
      "Epoch 37/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.7930 - acc: 0.7219 - val_loss: 0.7984 - val_acc: 0.7201\n",
      "\n",
      "Epoch 00037: val_loss did not improve from 0.71559\n",
      "Epoch 38/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.7875 - acc: 0.7248 - val_loss: 0.7603 - val_acc: 0.7315\n",
      "\n",
      "Epoch 00038: val_loss did not improve from 0.71559\n",
      "Epoch 39/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.7815 - acc: 0.7286 - val_loss: 0.7008 - val_acc: 0.7534\n",
      "\n",
      "Epoch 00039: val_loss improved from 0.71559 to 0.70075, saving model to model_best1.h5\n",
      "Epoch 40/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.7740 - acc: 0.7300 - val_loss: 0.7045 - val_acc: 0.7520\n",
      "\n",
      "Epoch 00040: val_loss did not improve from 0.70075\n",
      "Epoch 41/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.7699 - acc: 0.7326 - val_loss: 0.7271 - val_acc: 0.7454\n",
      "\n",
      "Epoch 00041: val_loss did not improve from 0.70075\n",
      "Epoch 42/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.7630 - acc: 0.7339 - val_loss: 0.6712 - val_acc: 0.7640\n",
      "\n",
      "Epoch 00042: val_loss improved from 0.70075 to 0.67123, saving model to model_best1.h5\n",
      "Epoch 43/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.7603 - acc: 0.7340 - val_loss: 0.7097 - val_acc: 0.7470\n",
      "\n",
      "Epoch 00043: val_loss did not improve from 0.67123\n",
      "Epoch 44/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.7516 - acc: 0.7385 - val_loss: 0.6812 - val_acc: 0.7622\n",
      "\n",
      "Epoch 00044: val_loss did not improve from 0.67123\n",
      "Epoch 45/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.7470 - acc: 0.7401 - val_loss: 0.6967 - val_acc: 0.7541\n",
      "\n",
      "Epoch 00045: val_loss did not improve from 0.67123\n",
      "Epoch 46/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.7382 - acc: 0.7437 - val_loss: 0.7710 - val_acc: 0.7357\n",
      "\n",
      "Epoch 00046: val_loss did not improve from 0.67123\n",
      "Epoch 47/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.7296 - acc: 0.7465 - val_loss: 0.7019 - val_acc: 0.7518\n",
      "\n",
      "Epoch 00047: val_loss did not improve from 0.67123\n",
      "Epoch 48/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.7334 - acc: 0.7448 - val_loss: 0.6364 - val_acc: 0.7769\n",
      "\n",
      "Epoch 00048: val_loss improved from 0.67123 to 0.63643, saving model to model_best1.h5\n",
      "Epoch 49/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.7234 - acc: 0.7497 - val_loss: 0.6737 - val_acc: 0.7663\n",
      "\n",
      "Epoch 00049: val_loss did not improve from 0.63643\n",
      "Epoch 50/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.7183 - acc: 0.7518 - val_loss: 0.6827 - val_acc: 0.7612\n",
      "\n",
      "Epoch 00050: val_loss did not improve from 0.63643\n",
      "Epoch 51/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.7135 - acc: 0.7524 - val_loss: 0.6914 - val_acc: 0.7593\n",
      "\n",
      "Epoch 00051: val_loss did not improve from 0.63643\n",
      "Epoch 52/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.7071 - acc: 0.7553 - val_loss: 0.6155 - val_acc: 0.7838\n",
      "\n",
      "Epoch 00052: val_loss improved from 0.63643 to 0.61550, saving model to model_best1.h5\n",
      "Epoch 53/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.7048 - acc: 0.7561 - val_loss: 0.6638 - val_acc: 0.7690\n",
      "\n",
      "Epoch 00053: val_loss did not improve from 0.61550\n",
      "Epoch 54/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.6924 - acc: 0.7603 - val_loss: 0.6669 - val_acc: 0.7681\n",
      "\n",
      "Epoch 00054: val_loss did not improve from 0.61550\n",
      "Epoch 55/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.6847 - acc: 0.7635 - val_loss: 0.6565 - val_acc: 0.7747\n",
      "\n",
      "Epoch 00055: val_loss did not improve from 0.61550\n",
      "Epoch 56/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.6838 - acc: 0.7638 - val_loss: 0.6205 - val_acc: 0.7847\n",
      "\n",
      "Epoch 00056: val_loss did not improve from 0.61550\n",
      "Epoch 57/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.6804 - acc: 0.7664 - val_loss: 0.6374 - val_acc: 0.7798\n",
      "\n",
      "Epoch 00057: val_loss did not improve from 0.61550\n",
      "Epoch 58/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.6743 - acc: 0.7669 - val_loss: 0.6745 - val_acc: 0.7626\n",
      "\n",
      "Epoch 00058: val_loss did not improve from 0.61550\n",
      "Epoch 59/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.6745 - acc: 0.7687 - val_loss: 0.6284 - val_acc: 0.7801\n",
      "\n",
      "Epoch 00059: val_loss did not improve from 0.61550\n",
      "Epoch 60/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.6727 - acc: 0.7670 - val_loss: 0.5991 - val_acc: 0.7909\n",
      "\n",
      "Epoch 00060: val_loss improved from 0.61550 to 0.59905, saving model to model_best1.h5\n",
      "Epoch 61/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.6659 - acc: 0.7703 - val_loss: 0.6348 - val_acc: 0.7815\n",
      "\n",
      "Epoch 00061: val_loss did not improve from 0.59905\n",
      "Epoch 62/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.6625 - acc: 0.7724 - val_loss: 0.6036 - val_acc: 0.7938\n",
      "\n",
      "Epoch 00062: val_loss did not improve from 0.59905\n",
      "Epoch 63/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.6592 - acc: 0.7732 - val_loss: 0.5725 - val_acc: 0.8001\n",
      "\n",
      "Epoch 00063: val_loss improved from 0.59905 to 0.57245, saving model to model_best1.h5\n",
      "Epoch 64/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.6560 - acc: 0.7727 - val_loss: 0.6830 - val_acc: 0.7669\n",
      "\n",
      "Epoch 00064: val_loss did not improve from 0.57245\n",
      "Epoch 65/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.6476 - acc: 0.7776 - val_loss: 0.6147 - val_acc: 0.7890\n",
      "\n",
      "Epoch 00065: val_loss did not improve from 0.57245\n",
      "Epoch 66/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.6425 - acc: 0.7764 - val_loss: 0.6152 - val_acc: 0.7893\n",
      "\n",
      "Epoch 00066: val_loss did not improve from 0.57245\n",
      "Epoch 67/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.6410 - acc: 0.7792 - val_loss: 0.5782 - val_acc: 0.8030\n",
      "\n",
      "Epoch 00067: val_loss did not improve from 0.57245\n",
      "Epoch 68/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.6304 - acc: 0.7837 - val_loss: 0.5867 - val_acc: 0.7975\n",
      "\n",
      "Epoch 00068: val_loss did not improve from 0.57245\n",
      "Epoch 69/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.6318 - acc: 0.7834 - val_loss: 0.6311 - val_acc: 0.7818\n",
      "\n",
      "Epoch 00069: val_loss did not improve from 0.57245\n",
      "Epoch 70/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.6264 - acc: 0.7840 - val_loss: 0.5640 - val_acc: 0.8052\n",
      "\n",
      "Epoch 00070: val_loss improved from 0.57245 to 0.56400, saving model to model_best1.h5\n",
      "Epoch 71/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.6244 - acc: 0.7851 - val_loss: 0.5463 - val_acc: 0.8155\n",
      "\n",
      "Epoch 00071: val_loss improved from 0.56400 to 0.54634, saving model to model_best1.h5\n",
      "Epoch 72/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.6271 - acc: 0.7835 - val_loss: 0.6575 - val_acc: 0.7760\n",
      "\n",
      "Epoch 00072: val_loss did not improve from 0.54634\n",
      "Epoch 73/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.6210 - acc: 0.7871 - val_loss: 0.5506 - val_acc: 0.8137\n",
      "\n",
      "Epoch 00073: val_loss did not improve from 0.54634\n",
      "Epoch 74/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.6135 - acc: 0.7890 - val_loss: 0.5792 - val_acc: 0.8008\n",
      "\n",
      "Epoch 00074: val_loss did not improve from 0.54634\n",
      "Epoch 75/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.6085 - acc: 0.7893 - val_loss: 0.5641 - val_acc: 0.8065\n",
      "\n",
      "Epoch 00075: val_loss did not improve from 0.54634\n",
      "Epoch 76/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.6018 - acc: 0.7929 - val_loss: 0.5668 - val_acc: 0.8071\n",
      "\n",
      "Epoch 00076: val_loss did not improve from 0.54634\n",
      "Epoch 77/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.6052 - acc: 0.7918 - val_loss: 0.5740 - val_acc: 0.8050\n",
      "\n",
      "Epoch 00077: val_loss did not improve from 0.54634\n",
      "Epoch 78/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.6064 - acc: 0.7918 - val_loss: 0.5550 - val_acc: 0.8106\n",
      "\n",
      "Epoch 00078: val_loss did not improve from 0.54634\n",
      "Epoch 79/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5987 - acc: 0.7931 - val_loss: 0.6081 - val_acc: 0.7936\n",
      "\n",
      "Epoch 00079: val_loss did not improve from 0.54634\n",
      "Epoch 80/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5973 - acc: 0.7942 - val_loss: 0.5368 - val_acc: 0.8151\n",
      "\n",
      "Epoch 00080: val_loss improved from 0.54634 to 0.53680, saving model to model_best1.h5\n",
      "Epoch 81/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5919 - acc: 0.7955 - val_loss: 0.5432 - val_acc: 0.8122\n",
      "\n",
      "Epoch 00081: val_loss did not improve from 0.53680\n",
      "Epoch 82/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5979 - acc: 0.7944 - val_loss: 0.5210 - val_acc: 0.8201\n",
      "\n",
      "Epoch 00082: val_loss improved from 0.53680 to 0.52103, saving model to model_best1.h5\n",
      "Epoch 83/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5812 - acc: 0.7990 - val_loss: 0.5325 - val_acc: 0.8174\n",
      "\n",
      "Epoch 00083: val_loss did not improve from 0.52103\n",
      "Epoch 84/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5836 - acc: 0.8008 - val_loss: 0.5269 - val_acc: 0.8197\n",
      "\n",
      "Epoch 00084: val_loss did not improve from 0.52103\n",
      "Epoch 85/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5852 - acc: 0.7990 - val_loss: 0.5423 - val_acc: 0.8138\n",
      "\n",
      "Epoch 00085: val_loss did not improve from 0.52103\n",
      "Epoch 86/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5821 - acc: 0.7985 - val_loss: 0.5402 - val_acc: 0.8189\n",
      "\n",
      "Epoch 00086: val_loss did not improve from 0.52103\n",
      "Epoch 87/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.5727 - acc: 0.8030 - val_loss: 0.5606 - val_acc: 0.8099\n",
      "\n",
      "Epoch 00087: val_loss did not improve from 0.52103\n",
      "Epoch 88/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5735 - acc: 0.8008 - val_loss: 0.5233 - val_acc: 0.8220\n",
      "\n",
      "Epoch 00088: val_loss did not improve from 0.52103\n",
      "Epoch 89/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5714 - acc: 0.8052 - val_loss: 0.5155 - val_acc: 0.8221\n",
      "\n",
      "Epoch 00089: val_loss improved from 0.52103 to 0.51549, saving model to model_best1.h5\n",
      "Epoch 90/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5673 - acc: 0.8056 - val_loss: 0.5712 - val_acc: 0.8090\n",
      "\n",
      "Epoch 00090: val_loss did not improve from 0.51549\n",
      "Epoch 91/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5688 - acc: 0.8034 - val_loss: 0.5208 - val_acc: 0.8213\n",
      "\n",
      "Epoch 00091: val_loss did not improve from 0.51549\n",
      "Epoch 92/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5671 - acc: 0.8057 - val_loss: 0.5776 - val_acc: 0.8046\n",
      "\n",
      "Epoch 00092: val_loss did not improve from 0.51549\n",
      "Epoch 93/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.5583 - acc: 0.8085 - val_loss: 0.5348 - val_acc: 0.8185\n",
      "\n",
      "Epoch 00093: val_loss did not improve from 0.51549\n",
      "Epoch 94/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5608 - acc: 0.8077 - val_loss: 0.5089 - val_acc: 0.8263\n",
      "\n",
      "Epoch 00094: val_loss improved from 0.51549 to 0.50885, saving model to model_best1.h5\n",
      "Epoch 95/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5547 - acc: 0.8104 - val_loss: 0.5637 - val_acc: 0.8063\n",
      "\n",
      "Epoch 00095: val_loss did not improve from 0.50885\n",
      "Epoch 96/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5534 - acc: 0.8105 - val_loss: 0.5449 - val_acc: 0.8133\n",
      "\n",
      "Epoch 00096: val_loss did not improve from 0.50885\n",
      "Epoch 97/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5496 - acc: 0.8135 - val_loss: 0.4954 - val_acc: 0.8336\n",
      "\n",
      "Epoch 00097: val_loss improved from 0.50885 to 0.49538, saving model to model_best1.h5\n",
      "Epoch 98/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5493 - acc: 0.8108 - val_loss: 0.5098 - val_acc: 0.8277\n",
      "\n",
      "Epoch 00098: val_loss did not improve from 0.49538\n",
      "Epoch 99/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5458 - acc: 0.8118 - val_loss: 0.5438 - val_acc: 0.8127\n",
      "\n",
      "Epoch 00099: val_loss did not improve from 0.49538\n",
      "Epoch 100/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5467 - acc: 0.8120 - val_loss: 0.5031 - val_acc: 0.8275\n",
      "\n",
      "Epoch 00100: val_loss did not improve from 0.49538\n",
      "Epoch 101/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5449 - acc: 0.8143 - val_loss: 0.4957 - val_acc: 0.8319\n",
      "\n",
      "Epoch 00101: val_loss did not improve from 0.49538\n",
      "Epoch 102/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5413 - acc: 0.8159 - val_loss: 0.5189 - val_acc: 0.8246\n",
      "\n",
      "Epoch 00102: val_loss did not improve from 0.49538\n",
      "Epoch 103/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5432 - acc: 0.8134 - val_loss: 0.4955 - val_acc: 0.8297\n",
      "\n",
      "Epoch 00103: val_loss did not improve from 0.49538\n",
      "Epoch 104/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5355 - acc: 0.8184 - val_loss: 0.4869 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00104: val_loss improved from 0.49538 to 0.48692, saving model to model_best1.h5\n",
      "Epoch 105/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5356 - acc: 0.8159 - val_loss: 0.5153 - val_acc: 0.8246\n",
      "\n",
      "Epoch 00105: val_loss did not improve from 0.48692\n",
      "Epoch 106/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5287 - acc: 0.8179 - val_loss: 0.4712 - val_acc: 0.8390\n",
      "\n",
      "Epoch 00106: val_loss improved from 0.48692 to 0.47122, saving model to model_best1.h5\n",
      "Epoch 107/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5294 - acc: 0.8201 - val_loss: 0.4802 - val_acc: 0.8358\n",
      "\n",
      "Epoch 00107: val_loss did not improve from 0.47122\n",
      "Epoch 108/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.5274 - acc: 0.8187 - val_loss: 0.4862 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00108: val_loss did not improve from 0.47122\n",
      "Epoch 109/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5259 - acc: 0.8206 - val_loss: 0.4876 - val_acc: 0.8350\n",
      "\n",
      "Epoch 00109: val_loss did not improve from 0.47122\n",
      "Epoch 110/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5163 - acc: 0.8246 - val_loss: 0.5011 - val_acc: 0.8298\n",
      "\n",
      "Epoch 00110: val_loss did not improve from 0.47122\n",
      "Epoch 111/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5185 - acc: 0.8225 - val_loss: 0.4900 - val_acc: 0.8362\n",
      "\n",
      "Epoch 00111: val_loss did not improve from 0.47122\n",
      "Epoch 112/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5227 - acc: 0.8212 - val_loss: 0.4498 - val_acc: 0.8470\n",
      "\n",
      "Epoch 00112: val_loss improved from 0.47122 to 0.44985, saving model to model_best1.h5\n",
      "Epoch 113/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5137 - acc: 0.8241 - val_loss: 0.5302 - val_acc: 0.8219\n",
      "\n",
      "Epoch 00113: val_loss did not improve from 0.44985\n",
      "Epoch 114/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.5132 - acc: 0.8258 - val_loss: 0.4824 - val_acc: 0.8379\n",
      "\n",
      "Epoch 00114: val_loss did not improve from 0.44985\n",
      "Epoch 115/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5162 - acc: 0.8230 - val_loss: 0.5060 - val_acc: 0.8312\n",
      "\n",
      "Epoch 00115: val_loss did not improve from 0.44985\n",
      "Epoch 116/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5155 - acc: 0.8242 - val_loss: 0.4814 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00116: val_loss did not improve from 0.44985\n",
      "Epoch 117/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5034 - acc: 0.8280 - val_loss: 0.4876 - val_acc: 0.8368\n",
      "\n",
      "Epoch 00117: val_loss did not improve from 0.44985\n",
      "Epoch 118/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.5060 - acc: 0.8254 - val_loss: 0.4818 - val_acc: 0.8363\n",
      "\n",
      "Epoch 00118: val_loss did not improve from 0.44985\n",
      "Epoch 119/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5086 - acc: 0.8252 - val_loss: 0.4857 - val_acc: 0.8375\n",
      "\n",
      "Epoch 00119: val_loss did not improve from 0.44985\n",
      "Epoch 120/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5014 - acc: 0.8283 - val_loss: 0.5383 - val_acc: 0.8177\n",
      "\n",
      "Epoch 00120: val_loss did not improve from 0.44985\n",
      "Epoch 121/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.4972 - acc: 0.8308 - val_loss: 0.4785 - val_acc: 0.8413\n",
      "\n",
      "Epoch 00121: val_loss did not improve from 0.44985\n",
      "Epoch 122/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.5019 - acc: 0.8292 - val_loss: 0.4668 - val_acc: 0.8416\n",
      "\n",
      "Epoch 00122: val_loss did not improve from 0.44985\n",
      "Epoch 123/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.4932 - acc: 0.8314 - val_loss: 0.5187 - val_acc: 0.8245\n",
      "\n",
      "Epoch 00123: val_loss did not improve from 0.44985\n",
      "Epoch 124/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.4981 - acc: 0.8282 - val_loss: 0.4864 - val_acc: 0.8351\n",
      "\n",
      "Epoch 00124: val_loss did not improve from 0.44985\n",
      "Epoch 125/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4913 - acc: 0.8308 - val_loss: 0.4708 - val_acc: 0.8409\n",
      "\n",
      "Epoch 00125: val_loss did not improve from 0.44985\n",
      "Epoch 126/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4927 - acc: 0.8315 - val_loss: 0.4576 - val_acc: 0.8441\n",
      "\n",
      "Epoch 00126: val_loss did not improve from 0.44985\n",
      "Epoch 127/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4899 - acc: 0.8335 - val_loss: 0.4588 - val_acc: 0.8464\n",
      "\n",
      "Epoch 00127: val_loss did not improve from 0.44985\n",
      "Epoch 128/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.4862 - acc: 0.8313 - val_loss: 0.4492 - val_acc: 0.8450\n",
      "\n",
      "Epoch 00128: val_loss improved from 0.44985 to 0.44919, saving model to model_best1.h5\n",
      "Epoch 129/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4928 - acc: 0.8301 - val_loss: 0.4468 - val_acc: 0.8491\n",
      "\n",
      "Epoch 00129: val_loss improved from 0.44919 to 0.44683, saving model to model_best1.h5\n",
      "Epoch 130/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4820 - acc: 0.8372 - val_loss: 0.4858 - val_acc: 0.8382\n",
      "\n",
      "Epoch 00130: val_loss did not improve from 0.44683\n",
      "Epoch 131/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4857 - acc: 0.8331 - val_loss: 0.4540 - val_acc: 0.8513\n",
      "\n",
      "Epoch 00131: val_loss did not improve from 0.44683\n",
      "Epoch 132/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4835 - acc: 0.8344 - val_loss: 0.4637 - val_acc: 0.8448\n",
      "\n",
      "Epoch 00132: val_loss did not improve from 0.44683\n",
      "Epoch 133/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4766 - acc: 0.8373 - val_loss: 0.4424 - val_acc: 0.8534\n",
      "\n",
      "Epoch 00133: val_loss improved from 0.44683 to 0.44239, saving model to model_best1.h5\n",
      "Epoch 134/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4799 - acc: 0.8372 - val_loss: 0.4874 - val_acc: 0.8366\n",
      "\n",
      "Epoch 00134: val_loss did not improve from 0.44239\n",
      "Epoch 135/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4788 - acc: 0.8367 - val_loss: 0.4572 - val_acc: 0.8471\n",
      "\n",
      "Epoch 00135: val_loss did not improve from 0.44239\n",
      "Epoch 136/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4746 - acc: 0.8363 - val_loss: 0.4750 - val_acc: 0.8413\n",
      "\n",
      "Epoch 00136: val_loss did not improve from 0.44239\n",
      "Epoch 137/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4790 - acc: 0.8374 - val_loss: 0.4565 - val_acc: 0.8468\n",
      "\n",
      "Epoch 00137: val_loss did not improve from 0.44239\n",
      "Epoch 138/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.4693 - acc: 0.8387 - val_loss: 0.4686 - val_acc: 0.8452\n",
      "\n",
      "Epoch 00138: val_loss did not improve from 0.44239\n",
      "Epoch 139/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.4750 - acc: 0.8361 - val_loss: 0.4267 - val_acc: 0.8553\n",
      "\n",
      "Epoch 00139: val_loss improved from 0.44239 to 0.42670, saving model to model_best1.h5\n",
      "Epoch 140/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.4731 - acc: 0.8390 - val_loss: 0.4493 - val_acc: 0.8477\n",
      "\n",
      "Epoch 00140: val_loss did not improve from 0.42670\n",
      "Epoch 141/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.4607 - acc: 0.8412 - val_loss: 0.4396 - val_acc: 0.8527\n",
      "\n",
      "Epoch 00141: val_loss did not improve from 0.42670\n",
      "Epoch 142/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4727 - acc: 0.8380 - val_loss: 0.4333 - val_acc: 0.8534\n",
      "\n",
      "Epoch 00142: val_loss did not improve from 0.42670\n",
      "Epoch 143/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.4711 - acc: 0.8390 - val_loss: 0.4807 - val_acc: 0.8394\n",
      "\n",
      "Epoch 00143: val_loss did not improve from 0.42670\n",
      "Epoch 144/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.4709 - acc: 0.8380 - val_loss: 0.4456 - val_acc: 0.8489\n",
      "\n",
      "Epoch 00144: val_loss did not improve from 0.42670\n",
      "Epoch 145/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4592 - acc: 0.8425 - val_loss: 0.4780 - val_acc: 0.8450\n",
      "\n",
      "Epoch 00145: val_loss did not improve from 0.42670\n",
      "Epoch 146/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4688 - acc: 0.8394 - val_loss: 0.4353 - val_acc: 0.8568\n",
      "\n",
      "Epoch 00146: val_loss did not improve from 0.42670\n",
      "Epoch 147/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4614 - acc: 0.8426 - val_loss: 0.4401 - val_acc: 0.8539\n",
      "\n",
      "Epoch 00147: val_loss did not improve from 0.42670\n",
      "Epoch 148/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4614 - acc: 0.8428 - val_loss: 0.4618 - val_acc: 0.8449\n",
      "\n",
      "Epoch 00148: val_loss did not improve from 0.42670\n",
      "Epoch 149/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4531 - acc: 0.8447 - val_loss: 0.4631 - val_acc: 0.8467\n",
      "\n",
      "Epoch 00149: val_loss did not improve from 0.42670\n",
      "Epoch 150/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4578 - acc: 0.8428 - val_loss: 0.4305 - val_acc: 0.8568\n",
      "\n",
      "Epoch 00150: val_loss did not improve from 0.42670\n",
      "Epoch 151/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.4573 - acc: 0.8438 - val_loss: 0.4396 - val_acc: 0.8510\n",
      "\n",
      "Epoch 00151: val_loss did not improve from 0.42670\n",
      "Epoch 152/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.4559 - acc: 0.8446 - val_loss: 0.4767 - val_acc: 0.8425\n",
      "\n",
      "Epoch 00152: val_loss did not improve from 0.42670\n",
      "Epoch 153/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4442 - acc: 0.8475 - val_loss: 0.4508 - val_acc: 0.8483\n",
      "\n",
      "Epoch 00153: val_loss did not improve from 0.42670\n",
      "Epoch 154/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.4536 - acc: 0.8427 - val_loss: 0.4105 - val_acc: 0.8596\n",
      "\n",
      "Epoch 00154: val_loss improved from 0.42670 to 0.41046, saving model to model_best1.h5\n",
      "Epoch 155/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4492 - acc: 0.8464 - val_loss: 0.4192 - val_acc: 0.8583\n",
      "\n",
      "Epoch 00155: val_loss did not improve from 0.41046\n",
      "Epoch 156/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4542 - acc: 0.8445 - val_loss: 0.4226 - val_acc: 0.8600\n",
      "\n",
      "Epoch 00156: val_loss did not improve from 0.41046\n",
      "Epoch 157/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4440 - acc: 0.8489 - val_loss: 0.4186 - val_acc: 0.8600\n",
      "\n",
      "Epoch 00157: val_loss did not improve from 0.41046\n",
      "Epoch 158/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4468 - acc: 0.8465 - val_loss: 0.4205 - val_acc: 0.8589\n",
      "\n",
      "Epoch 00158: val_loss did not improve from 0.41046\n",
      "Epoch 159/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.4422 - acc: 0.8467 - val_loss: 0.4395 - val_acc: 0.8538\n",
      "\n",
      "Epoch 00159: val_loss did not improve from 0.41046\n",
      "Epoch 160/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4462 - acc: 0.8467 - val_loss: 0.4461 - val_acc: 0.8514\n",
      "\n",
      "Epoch 00160: val_loss did not improve from 0.41046\n",
      "Epoch 161/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.4495 - acc: 0.8463 - val_loss: 0.4171 - val_acc: 0.8608\n",
      "\n",
      "Epoch 00161: val_loss did not improve from 0.41046\n",
      "Epoch 162/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.4370 - acc: 0.8499 - val_loss: 0.4316 - val_acc: 0.8569\n",
      "\n",
      "Epoch 00162: val_loss did not improve from 0.41046\n",
      "Epoch 163/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4398 - acc: 0.8493 - val_loss: 0.4221 - val_acc: 0.8597\n",
      "\n",
      "Epoch 00163: val_loss did not improve from 0.41046\n",
      "Epoch 164/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4382 - acc: 0.8505 - val_loss: 0.4382 - val_acc: 0.8569\n",
      "\n",
      "Epoch 00164: val_loss did not improve from 0.41046\n",
      "Epoch 165/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4396 - acc: 0.8495 - val_loss: 0.4124 - val_acc: 0.8627\n",
      "\n",
      "Epoch 00165: val_loss did not improve from 0.41046\n",
      "Epoch 166/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4328 - acc: 0.8522 - val_loss: 0.4169 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00166: val_loss did not improve from 0.41046\n",
      "Epoch 167/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4323 - acc: 0.8508 - val_loss: 0.4226 - val_acc: 0.8576\n",
      "\n",
      "Epoch 00167: val_loss did not improve from 0.41046\n",
      "Epoch 168/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4332 - acc: 0.8521 - val_loss: 0.3988 - val_acc: 0.8666\n",
      "\n",
      "Epoch 00168: val_loss improved from 0.41046 to 0.39882, saving model to model_best1.h5\n",
      "Epoch 169/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4323 - acc: 0.8519 - val_loss: 0.4343 - val_acc: 0.8558\n",
      "\n",
      "Epoch 00169: val_loss did not improve from 0.39882\n",
      "Epoch 170/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4290 - acc: 0.8524 - val_loss: 0.4265 - val_acc: 0.8580\n",
      "\n",
      "Epoch 00170: val_loss did not improve from 0.39882\n",
      "Epoch 171/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4365 - acc: 0.8526 - val_loss: 0.4523 - val_acc: 0.8520\n",
      "\n",
      "Epoch 00171: val_loss did not improve from 0.39882\n",
      "Epoch 172/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4342 - acc: 0.8516 - val_loss: 0.4342 - val_acc: 0.8536\n",
      "\n",
      "Epoch 00172: val_loss did not improve from 0.39882\n",
      "Epoch 173/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4292 - acc: 0.8545 - val_loss: 0.4191 - val_acc: 0.8598\n",
      "\n",
      "Epoch 00173: val_loss did not improve from 0.39882\n",
      "Epoch 174/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4301 - acc: 0.8528 - val_loss: 0.4061 - val_acc: 0.8641\n",
      "\n",
      "Epoch 00174: val_loss did not improve from 0.39882\n",
      "Epoch 175/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4277 - acc: 0.8528 - val_loss: 0.4324 - val_acc: 0.8594\n",
      "\n",
      "Epoch 00175: val_loss did not improve from 0.39882\n",
      "Epoch 176/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4196 - acc: 0.8545 - val_loss: 0.4010 - val_acc: 0.8663\n",
      "\n",
      "Epoch 00176: val_loss did not improve from 0.39882\n",
      "Epoch 177/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.4271 - acc: 0.8545 - val_loss: 0.4199 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00177: val_loss did not improve from 0.39882\n",
      "Epoch 178/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4229 - acc: 0.8545 - val_loss: 0.4064 - val_acc: 0.8672\n",
      "\n",
      "Epoch 00178: val_loss did not improve from 0.39882\n",
      "Epoch 179/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4258 - acc: 0.8542 - val_loss: 0.4267 - val_acc: 0.8572\n",
      "\n",
      "Epoch 00179: val_loss did not improve from 0.39882\n",
      "Epoch 180/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4251 - acc: 0.8543 - val_loss: 0.4208 - val_acc: 0.8609\n",
      "\n",
      "Epoch 00180: val_loss did not improve from 0.39882\n",
      "Epoch 181/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4160 - acc: 0.8596 - val_loss: 0.4081 - val_acc: 0.8656\n",
      "\n",
      "Epoch 00181: val_loss did not improve from 0.39882\n",
      "Epoch 182/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4166 - acc: 0.8553 - val_loss: 0.3984 - val_acc: 0.8656\n",
      "\n",
      "Epoch 00182: val_loss improved from 0.39882 to 0.39839, saving model to model_best1.h5\n",
      "Epoch 183/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.4198 - acc: 0.8564 - val_loss: 0.3923 - val_acc: 0.8692\n",
      "\n",
      "Epoch 00183: val_loss improved from 0.39839 to 0.39234, saving model to model_best1.h5\n",
      "Epoch 184/200\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.4174 - acc: 0.8584 - val_loss: 0.4245 - val_acc: 0.8614\n",
      "\n",
      "Epoch 00184: val_loss did not improve from 0.39234\n",
      "Epoch 185/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4171 - acc: 0.8571 - val_loss: 0.4088 - val_acc: 0.8624\n",
      "\n",
      "Epoch 00185: val_loss did not improve from 0.39234\n",
      "Epoch 186/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4185 - acc: 0.8573 - val_loss: 0.4147 - val_acc: 0.8620\n",
      "\n",
      "Epoch 00186: val_loss did not improve from 0.39234\n",
      "Epoch 187/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4146 - acc: 0.8574 - val_loss: 0.4493 - val_acc: 0.8535\n",
      "\n",
      "Epoch 00187: val_loss did not improve from 0.39234\n",
      "Epoch 188/200\n",
      "781/781 [==============================] - 25s 32ms/step - loss: 0.4163 - acc: 0.8575 - val_loss: 0.4079 - val_acc: 0.8635\n",
      "\n",
      "Epoch 00188: val_loss did not improve from 0.39234\n",
      "Epoch 189/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.4109 - acc: 0.8598 - val_loss: 0.3979 - val_acc: 0.8657\n",
      "\n",
      "Epoch 00189: val_loss did not improve from 0.39234\n",
      "Epoch 190/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4165 - acc: 0.8583 - val_loss: 0.4353 - val_acc: 0.8562\n",
      "\n",
      "Epoch 00190: val_loss did not improve from 0.39234\n",
      "Epoch 191/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4099 - acc: 0.8606 - val_loss: 0.3833 - val_acc: 0.8723\n",
      "\n",
      "Epoch 00191: val_loss improved from 0.39234 to 0.38331, saving model to model_best1.h5\n",
      "Epoch 192/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4087 - acc: 0.8587 - val_loss: 0.3980 - val_acc: 0.8684\n",
      "\n",
      "Epoch 00192: val_loss did not improve from 0.38331\n",
      "Epoch 193/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4097 - acc: 0.8595 - val_loss: 0.4034 - val_acc: 0.8668\n",
      "\n",
      "Epoch 00193: val_loss did not improve from 0.38331\n",
      "Epoch 194/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4101 - acc: 0.8589 - val_loss: 0.4281 - val_acc: 0.8591\n",
      "\n",
      "Epoch 00194: val_loss did not improve from 0.38331\n",
      "Epoch 195/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.4112 - acc: 0.8594 - val_loss: 0.4048 - val_acc: 0.8671\n",
      "\n",
      "Epoch 00195: val_loss did not improve from 0.38331\n",
      "Epoch 196/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4027 - acc: 0.8611 - val_loss: 0.3876 - val_acc: 0.8707\n",
      "\n",
      "Epoch 00196: val_loss did not improve from 0.38331\n",
      "Epoch 197/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4024 - acc: 0.8617 - val_loss: 0.4150 - val_acc: 0.8646\n",
      "\n",
      "Epoch 00197: val_loss did not improve from 0.38331\n",
      "Epoch 198/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4103 - acc: 0.8594 - val_loss: 0.4191 - val_acc: 0.8613\n",
      "\n",
      "Epoch 00198: val_loss did not improve from 0.38331\n",
      "Epoch 199/200\n",
      "781/781 [==============================] - 25s 33ms/step - loss: 0.4051 - acc: 0.8603 - val_loss: 0.4149 - val_acc: 0.8659\n",
      "\n",
      "Epoch 00199: val_loss did not improve from 0.38331\n",
      "Epoch 200/200\n",
      "781/781 [==============================] - 26s 33ms/step - loss: 0.4029 - acc: 0.8621 - val_loss: 0.3957 - val_acc: 0.8653\n",
      "\n",
      "Epoch 00200: val_loss did not improve from 0.38331\n",
      "10000/10000 [==============================] - 2s 206us/step\n",
      "> 86.530\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "Skeleton_2()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "I have tried two different approaches:\n",
    "\n",
    "1. using Transfer Learning(VGG19 model):\n",
    "    Accuracy on test set using transfer learning was 84.200.\n",
    "\n",
    "\n",
    "2. using CNN network with Augmentation and Normalization :\n",
    "    Accuracy on test set using this method was 86.530.\n",
    "\n",
    "\n",
    "As per results CNN model is performing better whereas both models can be made to perform slightly better by tweaking some parameters or training for more epochs.\n",
    "In transfer learning model resizing images to the size of images on which the VGG19 model is having pre trained weights could give significantly better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
